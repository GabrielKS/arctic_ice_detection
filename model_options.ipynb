{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import get_data\n",
    "import preprocess\n",
    "import base_models\n",
    "from base_models import load_base_modeled\n",
    "import learner\n",
    "import evaluate_results\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = load_base_modeled(base_models.vgg16_label, get_data.train_label)\n",
    "valid_ds = load_base_modeled(base_models.vgg16_label, get_data.valid_label)\n",
    "test_ds = load_base_modeled(base_models.vgg16_label, get_data.test_label)\n",
    "input_shape = train_ds.element_spec[0].shape[1:]\n",
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameters_to_string(model_name, n_epochs, batch_size, steps_per_epoch):\n",
    "    params = {\"model\": model_name, \"epochs\": n_epochs, \"batch-size\": batch_size, \"steps-per-epoch\": steps_per_epoch}\n",
    "    return \"_\".join([f\"{k}={params[k]}\" for k in params])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_figs(history, results, label):\n",
    "    fig,axs = plt.subplots(1, 3, figsize=(20, 6))\n",
    "    evaluate_results.training_accuracy_plot(history, model_label=label, ax=axs[0])\n",
    "    evaluate_results.training_loss_plot(history, model_label=label, ax=axs[1])\n",
    "    evaluate_results.confusion_matrix(results, model_label=label, ax=axs[2])\n",
    "    fig.suptitle(label)\n",
    "\n",
    "def evaluate_hyperparameters(model_fn, n_epochs, batch_size, steps_per_epoch, n_samples=1, print_samples=False, verbose=0):\n",
    "    label = hyperparameters_to_string(model_fn.__name__.replace(\"_\", \"-\"), n_epochs, batch_size, steps_per_epoch)\n",
    "    history, results = [], []\n",
    "    if batch_size is not None:\n",
    "        this_test_ds = test_ds.unbatch().batch(batch_size)\n",
    "        this_test_ds.file_paths = test_ds.file_paths\n",
    "    else: this_test_ds = test_ds\n",
    "\n",
    "    t0 = time.time()\n",
    "    for i in range(n_samples):\n",
    "        model = model_fn(input_shape)  # Recreate the model each time for independent samples\n",
    "        if print_samples: print(f\"{label} ROUND {i}/{n_samples}\")\n",
    "        this_history = learner.train(model, n_epochs, train_ds.repeat(), valid_ds,\n",
    "            steps_per_epoch=steps_per_epoch, verbose=verbose).history\n",
    "        history.append(this_history)\n",
    "        this_results = learner.test(model, this_test_ds, verbose=verbose)\n",
    "        results.append(this_results)\n",
    "        # evaluation_figs(this_history, this_results, label+\" \"+str(i))\n",
    "    t1 = time.time()\n",
    "    history = evaluate_results.combine_history(history)\n",
    "    results = evaluate_results.combine_results(results)\n",
    "    print(f\"{label} ({(t1-t0):.2f}s)\")\n",
    "    evaluate_results.print_test_results(results)\n",
    "    evaluation_figs(history, results, label)\n",
    "    evaluate_results.generate_misclass_files(results, model_label=label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment 1: effect of varying epochs vs. steps_per_epoch while keeping the product constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 30\n",
    "evaluate_hyperparameters(learner.original_addon, 50, 32, 14, n_samples=n_samples)\n",
    "evaluate_hyperparameters(learner.original_addon, 100, 32, 7, n_samples=n_samples)\n",
    "evaluate_hyperparameters(learner.original_addon, 25, 32, 28, n_samples=n_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, not really. Lowering the number of epochs makes the loss spikes much more evident, though this just seems to be how the averaging turns out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment 2: effect of varying batch size while keeping all else constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_hyperparameters(learner.original_addon, 50, 32, 14, n_samples=n_samples)\n",
    "evaluate_hyperparameters(learner.original_addon, 50, 16, 14, n_samples=n_samples)\n",
    "evaluate_hyperparameters(learner.original_addon, 50, 64, 14, n_samples=n_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, not much of an effect. Maybe slightly better with smaller batch sizes. We can just stick with the defaults then."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many epochs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spe = tf.data.experimental.cardinality(train_ds).numpy()/preprocess.augmentation_reps\n",
    "evaluate_hyperparameters(learner.original_addon, 50, None, spe, n_samples=3)\n",
    "evaluate_hyperparameters(learner.original_addon, 500, None, spe, n_samples=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_hyperparameters(learner.original_addon, 100, None, spe, n_samples=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it seems that 50 epochs is pretty good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c89208a319734647c21f9b42b63efd0eab8dd7529171cd12ac3e06dee0f92754"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('NOAA-new3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
