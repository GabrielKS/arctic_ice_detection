{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depending on your installation, you might have to adjust some settings to get these paths to work properly\n",
    "import pipeline.get_data as get_data\n",
    "import pipeline.preprocess as preprocess\n",
    "import pipeline.base_models as base_models\n",
    "from pipeline.base_models import load_base_modeled\n",
    "import pipeline.learner as learner\n",
    "from top_level_evaluation import evaluate_hyperparameters\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = load_base_modeled(base_models.vgg16_label, get_data.train_label)\n",
    "valid_ds = load_base_modeled(base_models.vgg16_label, get_data.valid_label)\n",
    "test_ds = load_base_modeled(base_models.vgg16_label, get_data.test_label)\n",
    "input_shape = train_ds.element_spec[0].shape[1:]\n",
    "print(input_shape)\n",
    "data = (train_ds, valid_ds, test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment 1: effect of varying epochs vs. steps_per_epoch while keeping the product constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 30\n",
    "evaluate_hyperparameters(learner.original_addon, data, 50, 32, 14, n_samples=n_samples)\n",
    "evaluate_hyperparameters(learner.original_addon, data, 100, 32, 7, n_samples=n_samples)\n",
    "evaluate_hyperparameters(learner.original_addon, data, 25, 32, 28, n_samples=n_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, not really. Lowering the number of epochs makes the loss spikes much more evident, though this just seems to be how the averaging turns out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment 2: effect of varying batch size while keeping all else constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_hyperparameters(learner.original_addon, data, 50, 32, 14, n_samples=n_samples)\n",
    "evaluate_hyperparameters(learner.original_addon, data, 50, 16, 14, n_samples=n_samples)\n",
    "evaluate_hyperparameters(learner.original_addon, data, 50, 64, 14, n_samples=n_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, not much of an effect. Maybe slightly better with smaller batch sizes. We can just stick with the defaults then."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many epochs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spe = tf.data.experimental.cardinality(train_ds).numpy()/preprocess.augmentation_reps\n",
    "evaluate_hyperparameters(learner.original_addon, data, 50, None, spe, n_samples=3)\n",
    "evaluate_hyperparameters(learner.original_addon, data, 500, None, spe, n_samples=3)\n",
    "evaluate_hyperparameters(learner.original_addon, data, 100, None, spe, n_samples=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it seems that 50 epochs is pretty good."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c89208a319734647c21f9b42b63efd0eab8dd7529171cd12ac3e06dee0f92754"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('NOAA-new3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
